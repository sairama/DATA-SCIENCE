{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lr_example3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('customer_churn.csv', inferSchema=True, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(['features','churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_churn , test_churn = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_churn = LogisticRegression(labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_churn_model = lr_churn.fit( train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sum = fitted_churn_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                628|                628|\n",
      "|   mean| 0.1767515923566879| 0.1480891719745223|\n",
      "| stddev|0.38176241442295106|0.35547148869706313|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_sum.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = fitted_churn_model.evaluate(test_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8787.39,1.0...|    1|[1.13995656017805...|[0.75767166331083...|       0.0|\n",
      "|[28.0,8670.98,0.0...|    0|[8.42778231510716...|[0.99978134184793...|       0.0|\n",
      "|[28.0,11128.95,1....|    0|[4.61075322253530...|[0.99015359069972...|       0.0|\n",
      "|[29.0,5900.78,1.0...|    0|[4.52069542172407...|[0.98923567763949...|       0.0|\n",
      "|[29.0,9378.24,0.0...|    0|[5.31459403835229...|[0.99510480542301...|       0.0|\n",
      "|[29.0,9617.59,0.0...|    0|[4.95541338832602...|[0.99300411985825...|       0.0|\n",
      "|[29.0,11274.46,1....|    0|[4.93515180984922...|[0.99286194864237...|       0.0|\n",
      "|[29.0,13255.05,1....|    0|[4.54558509568392...|[0.98949751227561...|       0.0|\n",
      "|[30.0,7960.64,1.0...|    1|[3.67470374873993...|[0.97527015615511...|       0.0|\n",
      "|[30.0,8403.78,1.0...|    0|[6.37768580165268...|[0.99830383152564...|       0.0|\n",
      "|[30.0,8677.28,1.0...|    0|[4.39557758935193...|[0.98781846414538...|       0.0|\n",
      "|[30.0,10183.98,1....|    0|[3.25916189244133...|[0.96300094040773...|       0.0|\n",
      "|[30.0,10744.14,1....|    1|[1.97961174806189...|[0.87863976814208...|       0.0|\n",
      "|[30.0,10960.52,1....|    0|[2.71064354486594...|[0.93765178144170...|       0.0|\n",
      "|[31.0,5387.75,0.0...|    0|[2.96112663373087...|[0.95078673737379...|       0.0|\n",
      "|[31.0,8829.83,1.0...|    0|[4.81934065493749...|[0.99199252947517...|       0.0|\n",
      "|[32.0,8617.98,1.0...|    1|[1.29697900321547...|[0.78532611520699...|       0.0|\n",
      "|[32.0,12547.91,0....|    0|[0.69947150460344...|[0.66807058752187...|       0.0|\n",
      "|[33.0,11370.28,1....|    0|[6.96859249487343...|[0.99905990820088...|       0.0|\n",
      "|[33.0,12638.51,1....|    0|[4.12469461549011...|[0.98408882728212...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_eval = BinaryClassificationEvaluator(labelCol='churn',rawPredictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = churn_eval.evaluate(pred_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7456806426763508"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lr_churn.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers = spark.read.csv('new_customers.csv', inferSchema=True, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_cus = assembler.transform(new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = final_model.transform(test_new_cus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Company','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
